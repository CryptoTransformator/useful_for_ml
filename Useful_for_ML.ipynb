{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Useful for ML.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66da60a6"
      },
      "source": [
        "\n",
        "# Start"
      ],
      "id": "66da60a6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff5632be"
      },
      "source": [
        "%%time\n",
        "tqdm.pandas()   #tqdm is used to show any code running with a progress bar."
      ],
      "id": "ff5632be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "897b9365"
      },
      "source": [
        "#import some necessary librairies\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import warnings\n",
        "def ignore_warn(*args, **kwargs):\n",
        "    pass\n",
        "warnings.warn = ignore_warn #ignore annoying warning\n",
        "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "#test = pd.read_csv('')\n",
        "#train = pd.read_csv('')\n",
        "#sample_submission = pd.read_csv('')"
      ],
      "id": "897b9365",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4c603ae"
      },
      "source": [
        "Problems with decoding on Kaggle,\n",
        "'utf-8' codec can't decode byte 0xe2 in position 29445: invalid continuation byte"
      ],
      "id": "b4c603ae"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4833b2b"
      },
      "source": [
        "# DEFINE ENCODING\n",
        "file = '../input/news-summary/news_summary.csv'\n",
        "import chardet\n",
        "with open(file, 'rb') as rawdata:\n",
        "    result = chardet.detect(rawdata.read(100000))\n",
        "result"
      ],
      "id": "a4833b2b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c026cf8a"
      },
      "source": [
        "# fILL iso decoding \n",
        "data = pd.read_csv(file,encoding='HERE')\n",
        "data.head()"
      ],
      "id": "c026cf8a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "510e54be"
      },
      "source": [
        "API with Kaggle for Jupiter Notebook"
      ],
      "id": "510e54be"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9c53e57"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive/data for colab'\n",
        "!kaggle competitions download -c jigsaw-multilingual-toxic-comment-classification\n",
        "!unzip \\*.zip && rm *.zip # for unpacking in Colab"
      ],
      "id": "d9c53e57",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7c14115"
      },
      "source": [
        "How read csv in ZIP"
      ],
      "id": "a7c14115"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a371ffab"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('path to zip', 'r') as zipf:\n",
        "    zipf.extractall('/content/nameforfolder')"
      ],
      "id": "a371ffab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b0e912e"
      },
      "source": [
        ""
      ],
      "id": "1b0e912e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2ace4e5"
      },
      "source": [
        "# Convert to float\n",
        "# Turn the item price into a float\n",
        "dollarizer = lambda x: float(x[1:-1])\n",
        "chipo.item_price = chipo.item_price.apply(dollarizer)"
      ],
      "id": "b2ace4e5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f12facd"
      },
      "source": [
        "How read tsv"
      ],
      "id": "5f12facd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56bb599b"
      },
      "source": [
        "food = pd.read_csv(\"D:/DATA SCIENCE learn courses/MY SUFFERING PANDAS/en.openfoodfacts.org.products.tsv\", sep='\\t')"
      ],
      "id": "56bb599b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bab26e08"
      },
      "source": [
        "# Analysys, graphs"
      ],
      "id": "bab26e08"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52dc5f1f"
      },
      "source": [
        "Very long by time but cool funckion for creation tabs"
      ],
      "id": "52dc5f1f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa0efcab"
      },
      "source": [
        "from pandas_profiling import ProfileReport\n",
        "profile = ProfileReport(titanik_data, title = 'Pandas Profiling Report')\n",
        "profile"
      ],
      "id": "aa0efcab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9678e0b6"
      },
      "source": [
        "if it is much columns to receive info about data we need input additional parametrs"
      ],
      "id": "9678e0b6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65ebf971"
      },
      "source": [
        "train.info(max_cols=120)"
      ],
      "id": "65ebf971",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bf21dc5"
      },
      "source": [
        "Show features regarding time, how features changed? "
      ],
      "id": "0bf21dc5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55502267"
      },
      "source": [
        "fig, ax1 = plt.subplots(figsize = (12, 8))\n",
        "\n",
        "breath_1 = train.loc[train['breath_id'] == 1]\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "ax1.plot(breath_1['time_step'], breath_1['pressure'], 'r-', label='pressure')\n",
        "ax1.plot(breath_1['time_step'], breath_1['u_in'], 'g-', label='u_in')\n",
        "ax2.plot(breath_1['time_step'], breath_1['u_out'], 'b-', label='u_out')\n",
        "\n",
        "ax1.set_xlabel('Timestep')\n",
        "\n",
        "ax1.legend(loc=(1.1, 0.8))\n",
        "ax2.legend(loc=(1.1, 0.7))\n",
        "plt.show()"
      ],
      "id": "55502267",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5083a53b"
      },
      "source": [
        "Show row with particural value"
      ],
      "id": "5083a53b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e1900ef"
      },
      "source": [
        "df.loc[df['item_cnt'].isin([1644.0])]"
      ],
      "id": "7e1900ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2517c99"
      },
      "source": [
        "Show row by index"
      ],
      "id": "d2517c99"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e03f03f"
      },
      "source": [
        "df.loc[8162053]"
      ],
      "id": "8e03f03f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4931f7e"
      },
      "source": [
        "# Show particular prices in different shops and sort values.\n",
        "chipo_one_product.loc[chipo_one_product['item_name'].isin(['Barbacoa Bowl'])].sort_values(by='item_price')"
      ],
      "id": "f4931f7e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3ddce15"
      },
      "source": [
        "Persents for feature"
      ],
      "id": "f3ddce15"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "152c6217"
      },
      "source": [
        "df_all.Class.value_counts(sort=False) / df_all.shape[0]*100"
      ],
      "id": "152c6217",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be7adcf2"
      },
      "source": [
        "#How many times did someone order more than one Canned Soda?\n",
        "len(chipo[(chipo['item_name'] == 'Canned Soda') & (chipo['quantity'] > 1)])"
      ],
      "id": "be7adcf2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ed8dc53"
      },
      "source": [
        "Show ratio missing data and correlation"
      ],
      "id": "8ed8dc53"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afbb5c54"
      },
      "source": [
        "missing_cols = ['f1', 'f2']\n",
        "\n",
        "fig, axes = plt.subplots(ncols=2, figsize=(17, 4), dpi=100)\n",
        "\n",
        "sns.barplot(x=train[missing_cols].isnull().sum().index, y=train[missing_cols].isnull().sum().values, ax=axes[0])\n",
        "sns.barplot(x=test[missing_cols].isnull().sum().index, y=test[missing_cols].isnull().sum().values, ax=axes[1])\n",
        "\n",
        "axes[0].set_ylabel('Missing Value Count', size=15, labelpad=20)\n",
        "axes[0].tick_params(axis='x', labelsize=15)\n",
        "axes[0].tick_params(axis='y', labelsize=15)\n",
        "axes[1].tick_params(axis='x', labelsize=15)\n",
        "axes[1].tick_params(axis='y', labelsize=15)\n",
        "\n",
        "axes[0].set_title('Training Set', fontsize=13)\n",
        "axes[1].set_title('Test Set', fontsize=13)\n",
        "\n",
        "plt.show()\n",
        "#Only if you need fill missing values to no+col\n",
        "for df in [train, test]:\n",
        "    for col in ['f1', 'f2']:\n",
        "        df[col] = df[col].fillna(f'no_{col}')"
      ],
      "id": "afbb5c54",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93ed4a9a"
      },
      "source": [
        "Plot with 2 features in one plot, i.e. together"
      ],
      "id": "93ed4a9a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5995ac32"
      },
      "source": [
        "plt.plot(df['date'] df['curs'], label='USD Rub rate')\n",
        "plt.plot(df['date'] df['price'], label='OIL rate')\n",
        "plt.legend()"
      ],
      "id": "5995ac32",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "921be644"
      },
      "source": [
        "Circle diagram and bar diagram for showing distribution"
      ],
      "id": "921be644"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbee4361"
      },
      "source": [
        "fig, axes = plt.subplots(ncols=2, figsize=(17, 4), dpi=100)\n",
        "plt.tight_layout()\n",
        "\n",
        "df_train.groupby('target').count()['id'].plot(kind='pie', ax=axes[0], labels=['Not Disaster (57%)', 'Disaster (43%)'])\n",
        "sns.countplot(x=df_train['target'], hue=df_train['target'], ax=axes[1])\n",
        "\n",
        "axes[0].set_ylabel('')\n",
        "axes[1].set_ylabel('')\n",
        "axes[1].set_xticklabels(['Not Disaster (4342)', 'Disaster (3271)'])\n",
        "axes[0].tick_params(axis='x', labelsize=15)\n",
        "axes[0].tick_params(axis='y', labelsize=15)\n",
        "axes[1].tick_params(axis='x', labelsize=15)\n",
        "axes[1].tick_params(axis='y', labelsize=15)\n",
        "\n",
        "axes[0].set_title('Target Distribution in Training Set', fontsize=13)\n",
        "axes[1].set_title('Target Count in Training Set', fontsize=13)\n",
        "\n",
        "plt.show()"
      ],
      "id": "fbee4361",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a789252"
      },
      "source": [
        "histogram distribution"
      ],
      "id": "7a789252"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf799104"
      },
      "source": [
        "fig = px.histogram(\n",
        "    train, \n",
        "    x=\"time_to_eruption\",\n",
        "    width=800,\n",
        "    height=500,\n",
        "    nbins=100,\n",
        "    title='Time to eruption distribution'\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "id": "bf799104",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7bb811d"
      },
      "source": [
        "histogram all"
      ],
      "id": "b7bb811d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ef1169f"
      },
      "source": [
        "fig = px.line(\n",
        "    train, \n",
        "    y=\"time_to_eruption\",\n",
        "    width=800,\n",
        "    height=500,\n",
        "    title='Time to eruption for all volcanos'\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "id": "3ef1169f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8922c5fe"
      },
      "source": [
        "Bar"
      ],
      "id": "8922c5fe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27440285"
      },
      "source": [
        "fig = px.bar(\n",
        "    absent_df, \n",
        "    x=\"Sensor\",\n",
        "    y='Missed sensors',\n",
        "    width=800,\n",
        "    height=500,\n",
        "    title='Number of missed sensors in training dataset'\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "id": "27440285",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcf8dba7"
      },
      "source": [
        "What files are in folder? Answer will be in list (That for that cases when folder have big amount of files)"
      ],
      "id": "bcf8dba7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8b3b524"
      },
      "source": [
        "train_frags = glob.glob(\"../input/predict-volcanic-eruptions-ingv-oe/train/*\")\n",
        "len(train_frags)"
      ],
      "id": "e8b3b524",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf8fa7e7"
      },
      "source": [
        "Prepearing data within big amount of files"
      ],
      "id": "cf8fa7e7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8979675"
      },
      "source": [
        "sensors = set()\n",
        "observations = set()\n",
        "nan_columns = list()\n",
        "missed_groups = list()\n",
        "for_df = list()\n",
        "\n",
        "for item in train_frags:\n",
        "    name = int(item.split('.')[-2].split('/')[-1])\n",
        "    at_least_one_missed = 0\n",
        "    frag = pd.read_csv(item)\n",
        "    missed_group = list()\n",
        "    missed_percents = list()\n",
        "    for col in frag.columns:\n",
        "        missed_percents.append(frag[col].isnull().sum() / len(frag))\n",
        "        if pd.isnull(frag[col]).all() == True:\n",
        "            at_least_one_missed = 1\n",
        "            nan_columns.append(col)\n",
        "            missed_group.append(col)\n",
        "    if len(missed_group) > 0:\n",
        "        missed_groups.append(missed_group)\n",
        "    sensors.add(len(frag.columns))\n",
        "    observations.add(len(frag))\n",
        "    for_df.append([name, at_least_one_missed] + missed_percents)\n",
        "print('Unique number of sensors: ', sensors)\n",
        "print('Unique number of observations: ', observations)\n",
        "print('Number of totaly missed sensors:', len(nan_columns))"
      ],
      "id": "a8979675",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87e61c52"
      },
      "source": [
        "# Outliers"
      ],
      "id": "87e61c52"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe3cab19"
      },
      "source": [
        "Analys of outliers"
      ],
      "id": "fe3cab19"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "344012c0"
      },
      "source": [
        "df.DebtRatio.quantile([.975]) # it give point which are last 2.5% values "
      ],
      "id": "344012c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f414bb9"
      },
      "source": [
        "build scatterplot, where i can see outliers"
      ],
      "id": "7f414bb9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3c90388"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\n",
        "plt.ylabel('SalePrice', fontsize=13)\n",
        "plt.xlabel('GrLivArea', fontsize=13)\n",
        "plt.show()"
      ],
      "id": "d3c90388",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74f197d5"
      },
      "source": [
        "#Deleting outliers\n",
        "train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n",
        "\n",
        "#Check the graphic again\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(train['GrLivArea'], train['SalePrice'])\n",
        "plt.ylabel('SalePrice', fontsize=13)\n",
        "plt.xlabel('GrLivArea', fontsize=13)\n",
        "plt.show()"
      ],
      "id": "74f197d5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dbed903"
      },
      "source": [
        "# Work with text"
      ],
      "id": "6dbed903"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83bdfee6"
      },
      "source": [
        "# Delete unproper info, duplicated in other column\n",
        "pre2['text'] = pre2['author'].str.cat(pre2['date'].str.cat(pre2['read_more'].str.cat(pre2['text'].str.cat(pre2['ctext'], sep = \" \"), sep =\" \"),sep= \" \"), sep = \" \")"
      ],
      "id": "83bdfee6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93e9c7a4"
      },
      "source": [
        ""
      ],
      "id": "93e9c7a4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ad76d9e"
      },
      "source": [
        "Count word in text"
      ],
      "id": "1ad76d9e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "783c237a"
      },
      "source": [
        "Vectorization "
      ],
      "id": "783c237a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03d4deca"
      },
      "source": [
        ""
      ],
      "id": "03d4deca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75348cc4"
      },
      "source": [
        ""
      ],
      "id": "75348cc4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67541253"
      },
      "source": [
        ""
      ],
      "id": "67541253",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e600e578"
      },
      "source": [
        "Decomposition"
      ],
      "id": "e600e578"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "789b330b"
      },
      "source": [
        "svd = TruncatedSVD(n_components=50, n_iter=5, random_state=0)\n",
        "truncated_tfidf = svd.fit_transform(tfidf_features)\n",
        "df_tfidf_col_name = [\"tfidf_\"+str(i) for i in range(50)]\n",
        "df_tfidf = pd.DataFrame(truncated_tfidf)\n",
        "df_tfidf.columns = df_tfidf_col_name\n",
        "df_all = df_all.join(df_tfidf)"
      ],
      "id": "789b330b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6a03aa0"
      },
      "source": [
        "Bag of words"
      ],
      "id": "a6a03aa0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f0ac332"
      },
      "source": [
        "count_vectorizer = CountVectorizer(min_df=1, ngram_range=(1,1))\n",
        "count_features = count_vectorizer.fit_transform(df_all['clean_text'])\n",
        "count_svd = TruncatedSVD(n_components=50, n_iter=5, random_state=10)\n",
        "count_bow = count_svd.fit_transform(count_features)\n",
        "df_bow_col_name = [\"bow_\"+str(i) for i in range(50)]\n",
        "df_bow = pd.DataFrame(count_bow)\n",
        "df_bow.columns = df_bow_col_name\n",
        "df_all = df_all.join(df_bow)"
      ],
      "id": "1f0ac332",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5b78f4c"
      },
      "source": [
        "Ngram"
      ],
      "id": "d5b78f4c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd67dc79"
      },
      "source": [
        "def generate_ngrams(text, n_gram=1):\n",
        "    token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n",
        "    ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
        "    return [' '.join(ngram) for ngram in ngrams]\n",
        "\n",
        "N = 100\n",
        "\n",
        "# Unigrams\n",
        "disaster_unigrams = defaultdict(int)\n",
        "nondisaster_unigrams = defaultdict(int)\n",
        "\n",
        "for tweet in df_train[DISASTER_TWEETS]['text']:\n",
        "    for word in generate_ngrams(tweet):\n",
        "        disaster_unigrams[word] += 1\n",
        "        \n",
        "for tweet in df_train[~DISASTER_TWEETS]['text']:\n",
        "    for word in generate_ngrams(tweet):\n",
        "        nondisaster_unigrams[word] += 1\n",
        "        \n",
        "df_disaster_unigrams = pd.DataFrame(sorted(disaster_unigrams.items(), key=lambda x: x[1])[::-1])\n",
        "df_nondisaster_unigrams = pd.DataFrame(sorted(nondisaster_unigrams.items(), key=lambda x: x[1])[::-1])\n",
        "\n",
        "# Bigrams\n",
        "disaster_bigrams = defaultdict(int)\n",
        "nondisaster_bigrams = defaultdict(int)\n",
        "\n",
        "for tweet in df_train[DISASTER_TWEETS]['text']:\n",
        "    for word in generate_ngrams(tweet, n_gram=2):\n",
        "        disaster_bigrams[word] += 1\n",
        "        \n",
        "for tweet in df_train[~DISASTER_TWEETS]['text']:\n",
        "    for word in generate_ngrams(tweet, n_gram=2):\n",
        "        nondisaster_bigrams[word] += 1\n",
        "        \n",
        "df_disaster_bigrams = pd.DataFrame(sorted(disaster_bigrams.items(), key=lambda x: x[1])[::-1])\n",
        "df_nondisaster_bigrams = pd.DataFrame(sorted(nondisaster_bigrams.items(), key=lambda x: x[1])[::-1])\n",
        "\n",
        "# Trigrams\n",
        "disaster_trigrams = defaultdict(int)\n",
        "nondisaster_trigrams = defaultdict(int)\n",
        "\n",
        "for tweet in df_train[DISASTER_TWEETS]['text']:\n",
        "    for word in generate_ngrams(tweet, n_gram=3):\n",
        "        disaster_trigrams[word] += 1\n",
        "        \n",
        "for tweet in df_train[~DISASTER_TWEETS]['text']:\n",
        "    for word in generate_ngrams(tweet, n_gram=3):\n",
        "        nondisaster_trigrams[word] += 1\n",
        "        \n",
        "df_disaster_trigrams = pd.DataFrame(sorted(disaster_trigrams.items(), key=lambda x: x[1])[::-1])\n",
        "df_nondisaster_trigrams = pd.DataFrame(sorted(nondisaster_trigrams.items(), key=lambda x: x[1])[::-1])"
      ],
      "id": "bd67dc79",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a7602bf"
      },
      "source": [
        "#Checking Ngrams\n",
        "fig, axes = plt.subplots(ncols=2, figsize=(18, 50), dpi=100)\n",
        "plt.tight_layout()\n",
        "\n",
        "sns.barplot(y=df_disaster_unigrams[0].values[:N], x=df_disaster_unigrams[1].values[:N], ax=axes[0], color='red')\n",
        "sns.barplot(y=df_nondisaster_unigrams[0].values[:N], x=df_nondisaster_unigrams[1].values[:N], ax=axes[1], color='green')\n",
        "\n",
        "for i in range(2):\n",
        "    axes[i].spines['right'].set_visible(False)\n",
        "    axes[i].set_xlabel('')\n",
        "    axes[i].set_ylabel('')\n",
        "    axes[i].tick_params(axis='x', labelsize=13)\n",
        "    axes[i].tick_params(axis='y', labelsize=13)\n",
        "\n",
        "axes[0].set_title(f'Top {N} most common unigrams in Disaster Tweets', fontsize=15)\n",
        "axes[1].set_title(f'Top {N} most common unigrams in Non-disaster Tweets', fontsize=15)\n",
        "\n",
        "plt.show()"
      ],
      "id": "7a7602bf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4d3bab8"
      },
      "source": [
        "Work with\n",
        "\n",
        "GloVe-300d-840B\n",
        "\n",
        "FastText-Crawl-300d-2M"
      ],
      "id": "b4d3bab8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11fd9830"
      },
      "source": [
        "When you have pre-trained embeddings, doing standard preprocessing steps might not be a good idea because some of the valuable information can be lost. It is better to get vocabulary as close to embeddings as possible. In order to do that, train vocab and test vocab are created by counting the words in tweets."
      ],
      "id": "11fd9830"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9e7c232"
      },
      "source": [
        "%%time\n",
        "\n",
        "glove_embeddings = np.load('../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl', allow_pickle=True)\n",
        "fasttext_embeddings = np.load('../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl', allow_pickle=True)"
      ],
      "id": "b9e7c232",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7d15e36"
      },
      "source": [
        "#Words in the intersection of vocab and embeddings are stored in covered along with their counts. Words in vocab that don't exist in embeddings are stored in oov along with their counts. n_covered and n_oov are total number of counts and they are used for calculating coverage percentages.\n",
        "def build_vocab(X):\n",
        "    \n",
        "    tweets = X.apply(lambda s: s.split()).values      \n",
        "    vocab = {}\n",
        "    \n",
        "    for tweet in tweets:\n",
        "        for word in tweet:\n",
        "            try:\n",
        "                vocab[word] += 1\n",
        "            except KeyError:\n",
        "                vocab[word] = 1                \n",
        "    return vocab\n",
        "\n",
        "def check_embeddings_coverage(X, embeddings):\n",
        "    \n",
        "    vocab = build_vocab(X)    \n",
        "    \n",
        "    covered = {}\n",
        "    oov = {}    \n",
        "    n_covered = 0\n",
        "    n_oov = 0\n",
        "    \n",
        "    for word in vocab:\n",
        "        try:\n",
        "            covered[word] = embeddings[word]\n",
        "            n_covered += vocab[word]\n",
        "        except:\n",
        "            oov[word] = vocab[word]\n",
        "            n_oov += vocab[word]\n",
        "            \n",
        "    vocab_coverage = len(covered) / len(vocab)\n",
        "    text_coverage = (n_covered / (n_covered + n_oov))\n",
        "    \n",
        "    sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
        "    return sorted_oov, vocab_coverage, text_coverage\n",
        "train_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(df_train['text'], glove_embeddings)\n",
        "test_glove_oov, test_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(df_test['text'], glove_embeddings)\n",
        "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_glove_vocab_coverage, train_glove_text_coverage))\n",
        "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_glove_vocab_coverage, test_glove_text_coverage))\n",
        "\n",
        "train_fasttext_oov, train_fasttext_vocab_coverage, train_fasttext_text_coverage = check_embeddings_coverage(df_train['text'], fasttext_embeddings)\n",
        "test_fasttext_oov, test_fasttext_vocab_coverage, test_fasttext_text_coverage = check_embeddings_coverage(df_test['text'], fasttext_embeddings)\n",
        "print('FastText Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_fasttext_vocab_coverage, train_fasttext_text_coverage))\n",
        "print('FastText Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_fasttext_vocab_coverage, test_fasttext_text_coverage))"
      ],
      "id": "d7d15e36",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd9a8ad8"
      },
      "source": [
        "# Linear models"
      ],
      "id": "fd9a8ad8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e0b5dcf"
      },
      "source": [
        " As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed."
      ],
      "id": "6e0b5dcf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd076141"
      },
      "source": [
        "Skeweness"
      ],
      "id": "fd076141"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cfe9a74"
      },
      "source": [
        "See at distribution and understand there is skew or not"
      ],
      "id": "7cfe9a74"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "056a459d"
      },
      "source": [
        "\n",
        "sns.distplot(train['SalePrice'] , fit=norm);\n",
        "\n",
        "# Get the fitted parameters used by the function\n",
        "(mu, sigma) = norm.fit(train['SalePrice'])\n",
        "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
        "\n",
        "#Now plot the distribution\n",
        "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
        "            loc='best')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('SalePrice distribution')\n",
        "\n",
        "#Get also the QQ-plot\n",
        "fig = plt.figure()\n",
        "res = stats.probplot(train['SalePrice'], plot=plt)\n",
        "plt.show()"
      ],
      "id": "056a459d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2226b59d"
      },
      "source": [
        "If it is skeweness we use Log-transformation of the target variable"
      ],
      "id": "2226b59d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1661071"
      },
      "source": [
        "#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n",
        "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
        "\n",
        "#Check the new distribution \n",
        "sns.distplot(train['SalePrice'] , fit=norm);\n",
        "\n",
        "# Get the fitted parameters used by the function\n",
        "(mu, sigma) = norm.fit(train['SalePrice'])\n",
        "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
        "\n",
        "#Now plot the distribution\n",
        "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
        "            loc='best')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('SalePrice distribution')\n",
        "\n",
        "#Get also the QQ-plot\n",
        "fig = plt.figure()\n",
        "res = stats.probplot(train['SalePrice'], plot=plt)\n",
        "plt.show()\n"
      ],
      "id": "f1661071",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bda3fb32"
      },
      "source": [
        "Skewed features"
      ],
      "id": "bda3fb32"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "578be77f"
      },
      "source": [
        "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
        "\n",
        "# Check the skew of all numerical features\n",
        "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
        "print(\"\\nSkew in numerical features: \\n\")\n",
        "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
        "skewness.head(10)"
      ],
      "id": "578be77f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b320ac23"
      },
      "source": [
        "Box Cox Transformation of (highly) skewed features"
      ],
      "id": "b320ac23"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27506839"
      },
      "source": [
        "skewness = skewness[abs(skewness) > 0.75]\n",
        "print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
        "\n",
        "from scipy.special import boxcox1p\n",
        "skewed_features = skewness.index\n",
        "lam = 0.15\n",
        "for feat in skewed_features:\n",
        "    #all_data[feat] += 1\n",
        "    all_data[feat] = boxcox1p(all_data[feat], lam)\n",
        "    \n",
        "#all_data[skewed_features] = np.log1p(all_data[skewed_features])"
      ],
      "id": "27506839",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6212da9c"
      },
      "source": [
        "# Features engineering"
      ],
      "id": "6212da9c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf016fac"
      },
      "source": [
        "SimpleImputer - replace missing values with the mean value along each column."
      ],
      "id": "cf016fac"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01dd86d5"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Imputation\n",
        "my_imputer = SimpleImputer()\n",
        "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
        "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
        "\n",
        "# Imputation removed column names; put them back\n",
        "imputed_X_train.columns = X_train.columns\n",
        "imputed_X_valid.columns = X_valid.columns"
      ],
      "id": "01dd86d5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ffc00a1"
      },
      "source": [
        "Example function for column"
      ],
      "id": "9ffc00a1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e698d13e"
      },
      "source": [
        "# Get names of columns with missing values\n",
        "cols_with_missing = [col for col in X_train.columns\n",
        "                     if X_train[col].isnull().any()]\n",
        "\n",
        "# Drop columns in training and validation data\n",
        "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
        "reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n",
        "\n",
        "# Make new columns indicating what will be imputed\n",
        "for col in cols_with_missing:\n",
        "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
        "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()"
      ],
      "id": "e698d13e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d350217f"
      },
      "source": [
        "# \"Cardinality\" means the number of unique values in a column\n",
        "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
        "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
        "                        X_train_full[cname].dtype == \"object\"]"
      ],
      "id": "d350217f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02ad1e2d"
      },
      "source": [
        "# Select numerical columns\n",
        "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]"
      ],
      "id": "02ad1e2d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bb6e9a1"
      },
      "source": [
        "Work with columns, libraries"
      ],
      "id": "0bb6e9a1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b95e9a6"
      },
      "source": [
        "for site_id in data.site_id.unique():\n",
        "  cancel_rate_nFCO.append({'site_id':str(site_id), 'cancel_rate_nFCO': len(data[(data.state_id.isin([4,5,6])) & (data.site_id == site_id)]) / len(data[data.is_first_client_order == 0])})\n",
        "cancel_rate_nFCO = pd.DataFrame(cancel_rate_nFCO)"
      ],
      "id": "6b95e9a6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72550510"
      },
      "source": [
        "Delete all rows with particular value"
      ],
      "id": "72550510"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d51da908"
      },
      "source": [
        "train = train.drop(train[train.pressure < 0].index)"
      ],
      "id": "d51da908",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1acc03c4"
      },
      "source": [
        "#For all models besides RandomForest, boostings we need skaling or standartisation our different skale numeric data. \n",
        "#For linear models and logistic regression it is necessarily.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(df)\n",
        "scaler.transform(df)\n",
        "pd.DataFrame(data=scaler.transform(df), columns=df.columns)"
      ],
      "id": "1acc03c4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9759901b"
      },
      "source": [
        "If one id have many observations and we want find max for some feature by id:"
      ],
      "id": "9759901b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d045c1e"
      },
      "source": [
        "# max value of u_in and u_out for each breath\n",
        "train['breath_id__u_in__max'] = train.groupby(['breath_id'])['u_in'].transform('max')\n",
        "# Also after that for model conveniantly add diffenerce between values:\n",
        "train['u_in_diff1'] = train['u_in'] - train['u_in_lag1']\n",
        "train['u_out_diff1'] = train['u_out'] - train['u_out_lag1']"
      ],
      "id": "4d045c1e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a42cca40"
      },
      "source": [
        "Often for creating new features for model need pandas function:"
      ],
      "id": "a42cca40"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05c2aff7"
      },
      "source": [
        "bitcoin['close_max7d'] = bitcoin['close'].shift(1).rolling(window=7).max()\n",
        "bitcoin['open_mean7d'] = bitcoin['open'].shift(1).rolling(window=7).mean()\n",
        "\n",
        "#Also often for big numeric features (special for scoring or financial fields) we need create \n",
        "#columns with data of recent days 1,2,3..14\n",
        "#For that we could use function\n",
        "for day in range(11, 15):\n",
        "    bitcoin[f'close_d{day}'] = bitcoin['close'].shift(day)"
      ],
      "id": "05c2aff7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5a1a190"
      },
      "source": [
        "# if colomn have many id but every id is meeting no one time, so we should take to attention other columns wil\n",
        "# cardinaly be changed after changing id. So lagging will be that:\n",
        "train['u_in_lag1'] = train.groupby('breath_id')['u_in'].shift(1)\n",
        "# after that we should change or delete missing new values "
      ],
      "id": "d5a1a190",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5076624"
      },
      "source": [
        ""
      ],
      "id": "d5076624",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79eca159"
      },
      "source": [
        "reduce size of the dataframe"
      ],
      "id": "79eca159"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fae912b"
      },
      "source": [
        "def downcast(df):\n",
        "    #reduce size of the dataframe\n",
        "    float_cols = [c for c in df if df[c].dtype in [\"float64\"]]\n",
        "    int_cols = [c for c in df if df[c].dtype in ['int64']]\n",
        "    df[float_cols] = df[float_cols].astype('float32')\n",
        "    df[int_cols] = df[int_cols].astype('int16')\n",
        "    return df\n",
        "df = downcast(df)"
      ],
      "id": "1fae912b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0297a03"
      },
      "source": [
        "Create column as first word another column"
      ],
      "id": "a0297a03"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7244e9f0"
      },
      "source": [
        "shops['shop_city'] = shops['shop_name'].str.split().str[0]"
      ],
      "id": "7244e9f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "517c2f97"
      },
      "source": [
        "Set date format"
      ],
      "id": "517c2f97"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72f4c9bf"
      },
      "source": [
        "sales['date'] = pd.to_datetime(sales.date,format='%d.%m.%Y')\n",
        "sales['weekday'] = sales.date.dt.dayofweek"
      ],
      "id": "72f4c9bf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "138d8097"
      },
      "source": [
        "Not all category names have a '-' between the main category and the subcategory. We create groups by extracting the part of the name prior to a non-letter character. "
      ],
      "id": "138d8097"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cc7cb19"
      },
      "source": [
        "categories['category_name'].str.extract(r'(^[\\w\\s]*)')"
      ],
      "id": "2cc7cb19",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ade09fa"
      },
      "source": [
        "#Select the teams that start with G\n",
        "euro12[euro12.Team.str.startswith('G')]"
      ],
      "id": "1ade09fa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5850fee7"
      },
      "source": [
        "# .loc is another way to slice, using the labels of the columns and indexesю Present only the Shooting Accuracy from England, Italy and Russia\n",
        "\n",
        "euro12.loc[euro12.Team.isin(['England', 'Italy', 'Russia']), ['Team','Shooting Accuracy']]"
      ],
      "id": "5850fee7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a066d382"
      },
      "source": [
        "#label encode group names\n",
        "categories['group_id']  = le.fit_transform(categories.group_name.values)\n",
        "categories.sample(5)"
      ],
      "id": "a066d382",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c89cfae7"
      },
      "source": [
        "Split by every word + other manipulations"
      ],
      "id": "c89cfae7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fa1c49b"
      },
      "source": [
        "results = Counter()\n",
        "items['item_name'].str.split().apply(results.update)\n",
        "\n",
        "words = []\n",
        "cnts = []\n",
        "for key, value in results.items():\n",
        "    words.append(key)\n",
        "    cnts.append(value)\n",
        "    \n",
        "counts = pd.DataFrame({'word':words,'count':cnts})\n",
        "common_words = counts.query('count>200').word.to_list()\n",
        "for word in common_words:\n",
        "    items[f'{word}_in_name'] = items['item_name'].str.contains(word).astype('int8')\n",
        "drop_cols = [\n",
        "    'item_id','category_id','item_name','item_name_first4',\n",
        "    'item_name_first6','item_name_first11',\n",
        "    'category_name','group_name','group_id'\n",
        "]\n",
        "items = items.drop(columns=drop_cols)"
      ],
      "id": "6fa1c49b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88fd3b1b"
      },
      "source": [
        "Delete duplicated columns"
      ],
      "id": "88fd3b1b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2604e9d"
      },
      "source": [
        "data = data.loc[:,~data.columns.duplicated()]"
      ],
      "id": "e2604e9d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0a48e60"
      },
      "source": [
        "# Rename column using number of column\n",
        "writing_out.rename(columns = {list(writing_out)[0]: 'number_contract'}, inplace = True)"
      ],
      "id": "f0a48e60",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe3b7213"
      },
      "source": [
        "Missing data"
      ],
      "id": "fe3b7213"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6660fa51"
      },
      "source": [
        "#Add columns with number of missing values in each row\n",
        "train['n_missing'] = train[features].isna().sum(axis=1)\n",
        "test['n_missing'] = test[features].isna().sum(axis=1)"
      ],
      "id": "6660fa51",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "601a1280"
      },
      "source": [
        "Replacing missing data with None\n"
      ],
      "id": "601a1280"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11d08ead"
      },
      "source": [
        "for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n",
        "    all_data[col] = all_data[col].fillna('None')"
      ],
      "id": "11d08ead",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c31b383"
      },
      "source": [
        "Replacing missing data to median"
      ],
      "id": "6c31b383"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89414057"
      },
      "source": [
        "#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\n",
        "all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n",
        "    lambda x: x.fillna(x.median()))"
      ],
      "id": "89414057",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71b4da9e"
      },
      "source": [
        "Replacing missing data to further value"
      ],
      "id": "71b4da9e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44d79111"
      },
      "source": [
        "df.fillna(method='ffil', inplace=True)"
      ],
      "id": "44d79111",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7603dcf8"
      },
      "source": [
        "Drop rows where aren`t some values"
      ],
      "id": "7603dcf8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b239f238"
      },
      "source": [
        "shops.dropna(inplace=True)"
      ],
      "id": "b239f238",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a833d183"
      },
      "source": [
        "Dropping"
      ],
      "id": "a833d183"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5da4a6a"
      },
      "source": [
        "#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\n",
        "train.drop(\"Id\", axis = 1, inplace = True)\n",
        "test.drop(\"Id\", axis = 1, inplace = True)\n",
        "\n",
        "#check again the data size after dropping the 'Id' variable\n",
        "print(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \n",
        "print(\"The test data size after dropping Id feature is : {} \".format(test.shape))"
      ],
      "id": "d5da4a6a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b186b75"
      },
      "source": [
        "Drop by index coulumn`s\n",
        "all_data = all_data.drop(all_data.columns[[-1]], axis=1)"
      ],
      "id": "8b186b75",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2d28fc8"
      },
      "source": [
        "Correlation map - heatmap"
      ],
      "id": "e2d28fc8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b1a7ecf"
      },
      "source": [
        "#Correlation map to see how features are correlated with SalePrice\n",
        "corrmat = train.corr()\n",
        "plt.subplots(figsize=(12,9))\n",
        "sns.heatmap(corrmat, vmax=0.9, square=True)"
      ],
      "id": "2b1a7ecf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f57c5c10"
      },
      "source": [
        "Transforming some numerical variables that are really categorical"
      ],
      "id": "f57c5c10"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18f34ca6"
      },
      "source": [
        "#MSSubClass=The building class\n",
        "all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n",
        "\n",
        "\n",
        "#Changing OverallCond into a categorical variable\n",
        "all_data['OverallCond'] = all_data['OverallCond'].astype(str)\n",
        "\n",
        "\n",
        "#Year and month sold are transformed into categorical features.\n",
        "all_data['YrSold'] = all_data['YrSold'].astype(str)\n",
        "all_data['MoSold'] = all_data['MoSold'].astype(str)\n"
      ],
      "id": "18f34ca6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb7a6b74"
      },
      "source": [
        "Label Encoding some categorical variables that may contain information in their ordering set"
      ],
      "id": "cb7a6b74"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81697a4f"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
        "        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
        "        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
        "        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n",
        "        'YrSold', 'MoSold')\n",
        "# process columns, apply LabelEncoder to categorical features\n",
        "for c in cols:\n",
        "    lbl = LabelEncoder() \n",
        "    lbl.fit(list(all_data[c].values)) \n",
        "    all_data[c] = lbl.transform(list(all_data[c].values))\n",
        "\n",
        "# shape        \n",
        "print('Shape all_data: {}'.format(all_data.shape))\n"
      ],
      "id": "81697a4f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe56e0c0"
      },
      "source": [
        "Getting dummy categorical features"
      ],
      "id": "fe56e0c0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23d8fd9e"
      },
      "source": [
        "df_all = pd.get_dummies(data, columns=['Class'], drop_first=None)"
      ],
      "id": "23d8fd9e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa752151"
      },
      "source": [
        "all_data = pd.get_dummies(all_data)\n",
        "print(all_data.shape)"
      ],
      "id": "fa752151",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17e93bbd"
      },
      "source": [
        "jOIN FEATURES"
      ],
      "id": "17e93bbd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc06fc7f"
      },
      "source": [
        "train = pd.merge(training_variants, training_text, how = 'left', on = 'ID').fillna('')"
      ],
      "id": "cc06fc7f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "382cb638"
      },
      "source": [
        "# Merge so to change respectively category from numerical data to text data defining catogory\n",
        "train.merge(names_catogories, on='item_id')"
      ],
      "id": "382cb638",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3ba8663"
      },
      "source": [
        "Split to pivots. Yet don`t know why it is nessesary, it looked in Kaggle about predict sales prices 1C, where autor splited proportions for every shop"
      ],
      "id": "f3ba8663"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "056bb07a"
      },
      "source": [
        "shops_cats = shops_cats.pivot(index='shop_id', columns=['category_id'])"
      ],
      "id": "056bb07a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea89fd37"
      },
      "source": [
        "How at merging fil missed values to 0"
      ],
      "id": "ea89fd37"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6254d22b"
      },
      "source": [
        "df = pd.merge(df, sales, on=['shop_id', 'item_id', 'date_block_num'], how='left').fillna(0)"
      ],
      "id": "6254d22b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce6cc7f2"
      },
      "source": [
        "jOIN ROWS\n"
      ],
      "id": "ce6cc7f2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ea6219b"
      },
      "source": [
        "df_all = pd.concat([train, test]).reset_index(drop=True)"
      ],
      "id": "5ea6219b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0447ef66"
      },
      "source": [
        "How after joining train and tast create matrix and vector?"
      ],
      "id": "0447ef66"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9248469"
      },
      "source": [
        "df_train = df_all.iloc[:train.shape[0]]\n",
        "X = df_train.iloc[:,4:]\n",
        "y = df_train['Class']\n",
        "df_test = df_all.iloc[train.shape[0]:]\n",
        "X_test = df_test.iloc[:,4:]"
      ],
      "id": "e9248469",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb7ea951"
      },
      "source": [
        "# Modelling"
      ],
      "id": "fb7ea951"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a632fb45"
      },
      "source": [
        "Import librairies"
      ],
      "id": "a632fb45"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abbaa3eb"
      },
      "source": [
        "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
        "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
        "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb"
      ],
      "id": "abbaa3eb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f6fcc0e"
      },
      "source": [
        "def rmse(y_true, y_pred):\n",
        "    return math.sqrt(mse(y_true, y_pred))\n",
        "print('Simple LGB model rmse: ', rmse(y_val, preds))"
      ],
      "id": "9f6fcc0e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74d730b4"
      },
      "source": [
        "XGBClassifier"
      ],
      "id": "74d730b4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbfbe507"
      },
      "source": [
        "#Depends from there is test set or not!!!\n",
        "#X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)\n",
        "X_train = X.copy()\n",
        "y_train = y.copy()"
      ],
      "id": "cbfbe507",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a976efd9"
      },
      "source": [
        "#Search best parametrs for XGBClassifier\n",
        "def score_func(estimator, X, y):\n",
        "    score1 = log_loss(y,estimator.predict(X,\n",
        "                           ntree_limit=estimator.best_ntree_limit),\n",
        "                          labels=list(range(1,10)))\n",
        "    return -score1\n",
        "\n",
        "xgb = XGBClassifier(\n",
        "    objective = 'multi:softprob',\n",
        "    eval_metric = 'mlogloss',\n",
        "    num_class = 9,\n",
        "    nthread=4,\n",
        "    seed=10\n",
        ")\n",
        "\n",
        "parameters = {\n",
        "    'max_depth': range (4, 7, 1),\n",
        "    'learning_rate': [0.1, 0.01, 0.05]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_grid=parameters,\n",
        "    scoring = score_func,\n",
        "    n_jobs = 10,\n",
        "    cv = 3,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "id": "a976efd9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81ae384a"
      },
      "source": [
        "GRIDSEARCH"
      ],
      "id": "81ae384a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6def8dc"
      },
      "source": [
        "model = KNeighborsReggressor() # or any else\n",
        "param_grid = {'n_neighbors': range(1,10)} # or any else. important if range is not big we can give range, if isn`t \n",
        "# better give set of parametr throught coma for picking up\n",
        "gs = GridSearchCV(model, param_grid, 'neg_mean_absolute_error', cv=3) # in end we indicate any metric and cv if need\n",
        "gs.fit(X_train, y_train)"
      ],
      "id": "e6def8dc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9cf8433"
      },
      "source": [
        "gs.best_score_\n",
        "gs.best_params_\n",
        "gs.best_estimator_ # I can save that estimator"
      ],
      "id": "b9cf8433",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b05a647"
      },
      "source": [
        "After that we can check GridSearch on test set"
      ],
      "id": "3b05a647"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19362c7a"
      },
      "source": [
        "model = gs.best_estimator_\n",
        "prediction = model.predict(X_test)\n",
        "print(\"MAE\", mean_absolute_error(y_test, prediction))"
      ],
      "id": "19362c7a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1d43212"
      },
      "source": [
        "XGBClassifier"
      ],
      "id": "c1d43212"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fdeed1d"
      },
      "source": [
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "xgb_param = {'objective': 'multi:softprob',\n",
        "          'eval_metric' : 'mlogloss',\n",
        "          'learning_rate' : 0.05,\n",
        "          'max_depth' : 5,\n",
        "          'num_class' : 9,\n",
        "          'nthread': 4,\n",
        "          'seed': 10}\n",
        "\n",
        "dtrain_xgb = xgb.DMatrix(X_train, label=y_train)\n",
        "\n",
        "xbg_result = xgb.cv(xgb_param, \n",
        "                    dtrain_xgb, \n",
        "                    num_boost_round=300, \n",
        "                    nfold=3,\n",
        "                    stratified=True, \n",
        "                    early_stopping_rounds=50, \n",
        "                    verbose_eval=100, \n",
        "                    show_stdv=True)"
      ],
      "id": "5fdeed1d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc686391"
      },
      "source": [
        "Choose the best round"
      ],
      "id": "bc686391"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dca7b52"
      },
      "source": [
        "num_round_xgb = len(xbg_result['test-mlogloss-mean'])\n",
        "print('num boost rounds xgb=' + str(num_round_xgb))"
      ],
      "id": "3dca7b52",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ce644f8"
      },
      "source": [
        "And train XGBClassifier"
      ],
      "id": "5ce644f8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5efe5e58"
      },
      "source": [
        "xgb_cl = xgb.train(xgb_param, dtrain_xgb, num_boost_round=num_round_xgb)\n",
        "#Подготавливаем тестовые данные, чтобы модель воспринимала формат входных тестовых данных как и у тренировочных\n",
        "xgtest = xgb.DMatrix(X_test)\n",
        "y_pred = xgb_cl.predict(xgtest)"
      ],
      "id": "5efe5e58",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9433584d"
      },
      "source": [
        ""
      ],
      "id": "9433584d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41870e32"
      },
      "source": [
        "Attentinon! Next cell for further models. Don`t forget paste it!"
      ],
      "id": "41870e32"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f94e148d"
      },
      "source": [
        "#Validation function\n",
        "n_folds = 5\n",
        "\n",
        "def rmsle_cv(model):\n",
        "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n",
        "    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
        "    return(rmse)"
      ],
      "id": "f94e148d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac5923a6"
      },
      "source": [
        "LASSO Regression"
      ],
      "id": "ac5923a6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86b07954"
      },
      "source": [
        "This model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline"
      ],
      "id": "86b07954"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0e34cb9"
      },
      "source": [
        "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))"
      ],
      "id": "c0e34cb9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9236aee3"
      },
      "source": [
        "score = rmsle_cv(lasso)\n",
        "print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
      ],
      "id": "9236aee3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5468e96"
      },
      "source": [
        "Elastic Net Regression"
      ],
      "id": "c5468e96"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5795f386"
      },
      "source": [
        "make robust to outliers"
      ],
      "id": "5795f386"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b88aea1e"
      },
      "source": [
        "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))"
      ],
      "id": "b88aea1e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d621de9"
      },
      "source": [
        "score = rmsle_cv(ENet)\n",
        "print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
      ],
      "id": "0d621de9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d5e4a16"
      },
      "source": [
        "Kernel Ridge Regression"
      ],
      "id": "5d5e4a16"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fe8f291"
      },
      "source": [
        "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)"
      ],
      "id": "2fe8f291",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3952c48"
      },
      "source": [
        "score = rmsle_cv(KRR)\n",
        "print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
      ],
      "id": "c3952c48",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88d4dc5c"
      },
      "source": [
        "Gradient Boosting Regression :"
      ],
      "id": "88d4dc5c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1daf220e"
      },
      "source": [
        "#With huber loss that makes it robust to outliers\n",
        "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
        "                                   max_depth=4, max_features='sqrt',\n",
        "                                   min_samples_leaf=15, min_samples_split=10, \n",
        "                                   loss='huber', random_state =5)"
      ],
      "id": "1daf220e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70eee11a"
      },
      "source": [
        "score = rmsle_cv(GBoost)\n",
        "print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
      ],
      "id": "70eee11a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fb3f40f"
      },
      "source": [
        "XGBoost "
      ],
      "id": "3fb3f40f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecef94b7"
      },
      "source": [
        "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
        "                             learning_rate=0.05, max_depth=3, \n",
        "                             min_child_weight=1.7817, n_estimators=2200,\n",
        "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
        "                             subsample=0.5213, silent=1,\n",
        "                             random_state =7, nthread = -1)"
      ],
      "id": "ecef94b7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78786fde"
      },
      "source": [
        "score = rmsle_cv(model_xgb)\n",
        "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
      ],
      "id": "78786fde",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4e0db38"
      },
      "source": [
        "LightGBM"
      ],
      "id": "c4e0db38"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91376be0"
      },
      "source": [
        "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
        "                              learning_rate=0.05, n_estimators=720,\n",
        "                              max_bin = 55, bagging_fraction = 0.8,\n",
        "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
        "                              feature_fraction_seed=9, bagging_seed=9,\n",
        "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)"
      ],
      "id": "91376be0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ae45e10"
      },
      "source": [
        "score = rmsle_cv(model_lgb)\n",
        "print(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))"
      ],
      "id": "4ae45e10",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05a41658"
      },
      "source": [
        "lgb"
      ],
      "id": "05a41658"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51e715c8"
      },
      "source": [
        "import lightgbm as lgb\n",
        "def build_lgb_model(params, X_train, X_val, y_train, y_val, cat_features):\n",
        "    lgb_train = lgb.Dataset(X_train, y_train)\n",
        "    lgb_val = lgb.Dataset(X_val, y_val)\n",
        "    model = lgb.train(params=params, train_set=lgb_train, valid_sets=(lgb_train, lgb_val), verbose_eval=50,\n",
        "                     categorical_feature=cat_features)\n",
        "    return model"
      ],
      "id": "51e715c8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50f4dcd6"
      },
      "source": [
        "params = {\n",
        "    'objective': 'rmse',\n",
        "    'metric': 'rmse',\n",
        "    'num_leaves': 1023,\n",
        "    'min_data_in_leaf':10,\n",
        "    'feature_fraction':0.7,\n",
        "    'learning_rate': 0.01,\n",
        "    'num_rounds': 1000,\n",
        "    'early_stopping_rounds': 30,\n",
        "    'seed': 1\n",
        "}\n",
        "#designating the categorical features which should be focused on\n",
        "cat_features = ['category_id','month','shop_id','shop_city']\n",
        "\n",
        "lgb_model = build_lgb_model(params, X_train, X_val, y_train, y_val, cat_features)\n",
        "\n",
        "#save model for later use\n",
        "lgb_model.save_model('initial_lgb_model.txt')"
      ],
      "id": "50f4dcd6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e60cf605"
      },
      "source": [
        "summary plot of feature importance"
      ],
      "id": "e60cf605"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "711bfffe"
      },
      "source": [
        "temp = X_test.drop(columns='lgb_pred').sample(10000)\n",
        "explainer = shap.TreeExplainer(lgb_model)\n",
        "shap_values = explainer.shap_values(temp)\n",
        "shap.summary_plot(shap_values, temp, max_display=30)"
      ],
      "id": "711bfffe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e6810c0"
      },
      "source": [
        "How enhance LGMN Regressor?"
      ],
      "id": "3e6810c0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32a33957"
      },
      "source": [
        "sampler = TPESampler(seed=666)\n",
        "\n",
        "def create_model(trial):\n",
        "    num_leaves = trial.suggest_int(\"num_leaves\", 2, 31)\n",
        "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 300)\n",
        "    max_depth = trial.suggest_int('max_depth', 3, 8)\n",
        "    min_child_samples = trial.suggest_int('min_child_samples', 100, 1200)\n",
        "    learning_rate = trial.suggest_uniform('learning_rate', 0.0001, 0.99)\n",
        "    min_data_in_leaf = trial.suggest_int('min_data_in_leaf', 5, 90)\n",
        "    bagging_fraction = trial.suggest_uniform('bagging_fraction', 0.0001, 1.0)\n",
        "    feature_fraction = trial.suggest_uniform('feature_fraction', 0.0001, 1.0)\n",
        "    model = LGBMRegressor(\n",
        "        num_leaves=num_leaves,\n",
        "        n_estimators=n_estimators, \n",
        "        max_depth=max_depth, \n",
        "        min_child_samples=min_child_samples, \n",
        "        min_data_in_leaf=min_data_in_leaf,\n",
        "        learning_rate=learning_rate,\n",
        "        feature_fraction=feature_fraction,\n",
        "        random_state=666\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def objective(trial):\n",
        "    model = create_model(trial)\n",
        "    model.fit(train, y)\n",
        "    preds = model.predict(val)\n",
        "    score = rmse(y_val, preds)\n",
        "    return score\n",
        "\n",
        "# To use optuna uncomment it \n",
        "# study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
        "# study.optimize(objective, n_trials=5000)\n",
        "# params = study.best_params\n",
        "# params['random_state'] = 666\n",
        "\n",
        "params = {\n",
        "    'num_leaves': 29,\n",
        "    'n_estimators': 289,\n",
        "    'max_depth': 8,\n",
        "    'min_child_samples': 507,\n",
        "    'learning_rate': 0.0812634327662599,\n",
        "    'min_data_in_leaf': 13,\n",
        "    'bagging_fraction': 0.020521665677937423,\n",
        "    'feature_fraction': 0.05776459974779927,\n",
        "    'random_state': 666\n",
        "}\n",
        "\n",
        "lgb = LGBMRegressor(**params)\n",
        "lgb.fit(train, y)"
      ],
      "id": "32a33957",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a422d66c"
      },
      "source": [
        "parms = {\n",
        "    'num_leaves': 31, \n",
        "    'n_estimators': 138, \n",
        "    'max_depth': 8, \n",
        "    'min_child_samples': 182, \n",
        "    'learning_rate': 0.16630987899513125, \n",
        "    'min_data_in_leaf': 24, \n",
        "    'bagging_fraction': 0.8743237361979733, \n",
        "    'feature_fraction': 0.45055692472636766,\n",
        "    'random_state': 666\n",
        "}\n",
        "\n",
        "rfe_lgb = RFE(\n",
        "    estimator=DecisionTreeRegressor(\n",
        "        random_state=666\n",
        "    ), \n",
        "    n_features_to_select=83\n",
        ")\n",
        "\n",
        "pipe_lgb = Pipeline(\n",
        "    steps=[\n",
        "        ('s', rfe_lgb), \n",
        "        ('m', LGBMRegressor(**parms))\n",
        "    ]\n",
        ")\n",
        "\n",
        "pipe_lgb.fit(train, y)\n",
        "preds = pipe_lgb.predict(val)  \n",
        "print('LGB rmse', rmse(y_val, preds))"
      ],
      "id": "a422d66c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb61107d"
      },
      "source": [
        "Building features if we have many files csv"
      ],
      "id": "cb61107d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86972c67"
      },
      "source": [
        "def build_features(signal, ts, sensor_id):\n",
        "    X = pd.DataFrame()\n",
        "    f = np.fft.fft(signal)\n",
        "    f_real = np.real(f)\n",
        "    X.loc[ts, f'{sensor_id}_sum']       = signal.sum()\n",
        "    X.loc[ts, f'{sensor_id}_mean']      = signal.mean()\n",
        "    X.loc[ts, f'{sensor_id}_std']       = signal.std()\n",
        "    X.loc[ts, f'{sensor_id}_var']       = signal.var() \n",
        "    X.loc[ts, f'{sensor_id}_max']       = signal.max()\n",
        "    X.loc[ts, f'{sensor_id}_min']       = signal.min()\n",
        "    X.loc[ts, f'{sensor_id}_skew']      = signal.skew()\n",
        "    X.loc[ts, f'{sensor_id}_mad']       = signal.mad()\n",
        "    X.loc[ts, f'{sensor_id}_kurtosis']  = signal.kurtosis()\n",
        "    X.loc[ts, f'{sensor_id}_quantile99']= np.quantile(signal, 0.99)\n",
        "    X.loc[ts, f'{sensor_id}_quantile95']= np.quantile(signal, 0.95)\n",
        "    X.loc[ts, f'{sensor_id}_quantile85']= np.quantile(signal, 0.85)\n",
        "    X.loc[ts, f'{sensor_id}_quantile75']= np.quantile(signal, 0.75)\n",
        "    X.loc[ts, f'{sensor_id}_quantile55']= np.quantile(signal, 0.55)\n",
        "    X.loc[ts, f'{sensor_id}_quantile45']= np.quantile(signal, 0.45) \n",
        "    X.loc[ts, f'{sensor_id}_quantile25']= np.quantile(signal, 0.25) \n",
        "    X.loc[ts, f'{sensor_id}_quantile15']= np.quantile(signal, 0.15) \n",
        "    X.loc[ts, f'{sensor_id}_quantile05']= np.quantile(signal, 0.05)\n",
        "    X.loc[ts, f'{sensor_id}_quantile01']= np.quantile(signal, 0.01)\n",
        "    X.loc[ts, f'{sensor_id}_fft_real_mean']= f_real.mean()\n",
        "    X.loc[ts, f'{sensor_id}_fft_real_std'] = f_real.std()\n",
        "    X.loc[ts, f'{sensor_id}_fft_real_max'] = f_real.max()\n",
        "    X.loc[ts, f'{sensor_id}_fft_real_min'] = f_real.min()\n",
        "\n",
        "    return X"
      ],
      "id": "86972c67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4894a387"
      },
      "source": [
        "train_set = list()\n",
        "j=0\n",
        "for seg in train.segment_id:\n",
        "    signals = pd.read_csv(f'/kaggle/input/predict-volcanic-eruptions-ingv-oe/train/{seg}.csv')\n",
        "    train_row = []\n",
        "    if j%500 == 0:\n",
        "        print(j)\n",
        "    for i in range(0, 10):\n",
        "        sensor_id = f'sensor_{i+1}'\n",
        "        train_row.append(build_features(signals[sensor_id].fillna(0), seg, sensor_id))\n",
        "    train_row = pd.concat(train_row, axis=1)\n",
        "    train_set.append(train_row)\n",
        "    j+=1\n",
        "\n",
        "train_set = pd.concat(train_set)"
      ],
      "id": "4894a387",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60742607"
      },
      "source": [
        "train_set = train_set.reset_index()\n",
        "train_set = train_set.rename(columns={'index': 'segment_id'})\n",
        "train_set = pd.merge(train_set, train, on='segment_id')\n",
        "train_set"
      ],
      "id": "60742607",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a81bd57b"
      },
      "source": [
        "drop_cols = list()\n",
        "for col in train_set.columns:\n",
        "    if col == 'segment_id':\n",
        "        continue\n",
        "    if abs(train_set[col].corr(train_set['time_to_eruption'])) < 0.01:\n",
        "        drop_cols.append(col)"
      ],
      "id": "a81bd57b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e7b7610"
      },
      "source": [
        "not_to_drop_cols = list()\n",
        "\n",
        "for col1 in train_set.columns:\n",
        "    for col2 in train_set.columns:\n",
        "        if col1 == col2:\n",
        "            continue\n",
        "        if col1 == 'segment_id' or col2 == 'segment_id': \n",
        "            continue\n",
        "        if col1 == 'time_to_eruption' or col2 == 'time_to_eruption':\n",
        "            continue\n",
        "        if abs(train_set[col1].corr(train_set[col2])) > 0.98:\n",
        "            if col2 not in drop_cols and col1 not in not_to_drop_cols:\n",
        "                drop_cols.append(col2)\n",
        "                not_to_drop_cols.append(col1)"
      ],
      "id": "4e7b7610",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "944387c1"
      },
      "source": [
        "Another way to aggregate statistics"
      ],
      "id": "944387c1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64768693"
      },
      "source": [
        "def agg_stats(df, idx):\n",
        "    df = df.agg(['sum', 'min', \"mean\", \"std\", \"median\", \"skew\", \"kurtosis\"])\n",
        "    df_flat = df.stack()\n",
        "    df_flat.index = df_flat.index.map('{0[1]}_{0[0]}'.format)\n",
        "    df_out = df_flat.to_frame().T\n",
        "    df_out[\"segment_id\"] = int(idx)\n",
        "    return df_out"
      ],
      "id": "64768693",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc4aabbd"
      },
      "source": [
        "summary_stats = pd.DataFrame()\n",
        "for csv in tqdm(Path(\"../input/predict-volcanic-eruptions-ingv-oe/train/\").glob(\"**/*.csv\"), total=4501):\n",
        "    df = pd.read_csv(csv)\n",
        "    summary_stats = summary_stats.append(agg_stats(df, csv.stem))"
      ],
      "id": "bc4aabbd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7330e92"
      },
      "source": [
        "test_data = pd.DataFrame()\n",
        "for csv in tqdm(Path(\"../input/predict-volcanic-eruptions-ingv-oe/test/\").glob(\"**/*.csv\"), total=4501):\n",
        "    df = pd.read_csv(csv)\n",
        "    test_data = test_data.append(agg_stats(df, csv.stem))"
      ],
      "id": "a7330e92",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ba9924f"
      },
      "source": [
        "features = list(summary_stats.drop([\"segment_id\"], axis=1).columns)\n",
        "target_name = [\"time_to_eruption\"]\n",
        "summary_stats = summary_stats.merge(train, on=\"segment_id\")\n",
        "summary_stats.head()"
      ],
      "id": "2ba9924f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aaca5e1"
      },
      "source": [
        "Train test split"
      ],
      "id": "2aaca5e1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8e26598"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666, test_size=0.2, shuffle=True)"
      ],
      "id": "d8e26598",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "454cb45e"
      },
      "source": [
        "# Stacking models"
      ],
      "id": "454cb45e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d99ee30f"
      },
      "source": [
        "Averaged base models class"
      ],
      "id": "d99ee30f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14d39c99"
      },
      "source": [
        "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
        "    def __init__(self, models):\n",
        "        self.models = models\n",
        "        \n",
        "    # we define clones of the original models to fit the data in\n",
        "    def fit(self, X, y):\n",
        "        self.models_ = [clone(x) for x in self.models]\n",
        "        \n",
        "        # Train cloned base models\n",
        "        for model in self.models_:\n",
        "            model.fit(X, y)\n",
        "\n",
        "        return self\n",
        "    \n",
        "    #Now we do the predictions for cloned models and average them\n",
        "    def predict(self, X):\n",
        "        predictions = np.column_stack([\n",
        "            model.predict(X) for model in self.models_\n",
        "        ])\n",
        "        return np.mean(predictions, axis=1)   "
      ],
      "id": "14d39c99",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05de202d"
      },
      "source": [
        "Averaged base models score"
      ],
      "id": "05de202d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9caffc80"
      },
      "source": [
        "#We just average four models here ENet, GBoost, KRR and lasso. Of course we could easily add more models in the mix.\n",
        "averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n",
        "\n",
        "score = rmsle_cv(averaged_models)\n",
        "print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
      ],
      "id": "9caffc80",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3265570b"
      },
      "source": [
        "Adding a Meta-model"
      ],
      "id": "3265570b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc28eeec"
      },
      "source": [
        "Stacking averaged Models Class"
      ],
      "id": "cc28eeec"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e1b37f9"
      },
      "source": [
        "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
        "    def __init__(self, base_models, meta_model, n_folds=5):\n",
        "        self.base_models = base_models\n",
        "        self.meta_model = meta_model\n",
        "        self.n_folds = n_folds\n",
        "   \n",
        "    # We again fit the data on clones of the original models\n",
        "    def fit(self, X, y):\n",
        "        self.base_models_ = [list() for x in self.base_models]\n",
        "        self.meta_model_ = clone(self.meta_model)\n",
        "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
        "        \n",
        "        # Train cloned base models then create out-of-fold predictions\n",
        "        # that are needed to train the cloned meta-model\n",
        "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
        "        for i, model in enumerate(self.base_models):\n",
        "            for train_index, holdout_index in kfold.split(X, y):\n",
        "                instance = clone(model)\n",
        "                self.base_models_[i].append(instance)\n",
        "                instance.fit(X[train_index], y[train_index])\n",
        "                y_pred = instance.predict(X[holdout_index])\n",
        "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
        "                \n",
        "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
        "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
        "        return self\n",
        "   \n",
        "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
        "    #meta-features for the final prediction which is done by the meta-model\n",
        "    def predict(self, X):\n",
        "        meta_features = np.column_stack([\n",
        "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
        "            for base_models in self.base_models_ ])\n",
        "        return self.meta_model_.predict(meta_features)"
      ],
      "id": "1e1b37f9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3e9304a"
      },
      "source": [
        "Stacking Averaged models Score"
      ],
      "id": "f3e9304a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "566f3cd9"
      },
      "source": [
        "#To make the two approaches comparable (by using the same number of models) , we just average Enet KRR and Gboost, then we add lasso as meta-model.\n",
        "stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n",
        "                                                 meta_model = lasso)\n",
        "\n",
        "score = rmsle_cv(stacked_averaged_models)\n",
        "print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))"
      ],
      "id": "566f3cd9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a37a6277"
      },
      "source": [
        "Ensembling StackedRegressor, XGBoost and LightGBM"
      ],
      "id": "a37a6277"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbce9df2"
      },
      "source": [
        "def rmsle(y, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y, y_pred))"
      ],
      "id": "dbce9df2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ae15b02"
      },
      "source": [
        "#StackedRegressor:\n",
        "stacked_averaged_models.fit(train.values, y_train)\n",
        "stacked_train_pred = stacked_averaged_models.predict(train.values)\n",
        "stacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\n",
        "print(rmsle(y_train, stacked_train_pred))"
      ],
      "id": "3ae15b02",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1e0976f"
      },
      "source": [
        "#XGBoost:\n",
        "model_xgb.fit(train, y_train)\n",
        "xgb_train_pred = model_xgb.predict(train)\n",
        "xgb_pred = np.expm1(model_xgb.predict(test))\n",
        "print(rmsle(y_train, xgb_train_pred))"
      ],
      "id": "f1e0976f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ec03a44"
      },
      "source": [
        "#LightGBM\n",
        "model_lgb.fit(train, y_train)\n",
        "lgb_train_pred = model_lgb.predict(train)\n",
        "lgb_pred = np.expm1(model_lgb.predict(test.values))\n",
        "print(rmsle(y_train, lgb_train_pred))"
      ],
      "id": "7ec03a44",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cb8ccc5"
      },
      "source": [
        "'''RMSE on the entire Train data when averaging'''\n",
        "\n",
        "print('RMSLE score on train data:')\n",
        "print(rmsle(y_train,stacked_train_pred*0.70 +\n",
        "               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))"
      ],
      "id": "7cb8ccc5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fca7fd3f"
      },
      "source": [
        "Ensemble prediction:"
      ],
      "id": "fca7fd3f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc63e7cd"
      },
      "source": [
        "ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15"
      ],
      "id": "fc63e7cd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5065a7cb"
      },
      "source": [
        "Tester program that automatically runs k-fold cross-validation on scikit models accross different datasets."
      ],
      "id": "5065a7cb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe928bd1"
      },
      "source": [
        "# A utility class to test all of our models on different datasets\n",
        "class Tester():\n",
        "    def __init__(self, target):\n",
        "        self.target = target\n",
        "        self.datasets = {}\n",
        "        self.models = {}\n",
        "        self.cache = {} # we added a simple cache to speed things up\n",
        "\n",
        "    def addDataset(self, name, df):\n",
        "        self.datasets[name] = df.copy()\n",
        "\n",
        "    def addModel(self, name, model):\n",
        "        self.models[name] = model\n",
        "        \n",
        "    def clearModels(self):\n",
        "        self.models = {}\n",
        "\n",
        "    def clearCache(self):\n",
        "        self.cache = {}\n",
        "    \n",
        "    def testModelWithDataset(self, m_name, df_name, sample_len, cv):\n",
        "        if (m_name, df_name, sample_len, cv) in self.cache:\n",
        "            return self.cache[(m_name, df_name, sample_len, cv)]\n",
        "\n",
        "        clf = self.models[m_name]\n",
        "        \n",
        "        if not sample_len: \n",
        "            sample = self.datasets[df_name]\n",
        "        else: sample = self.datasets[df_name].sample(sample_len)\n",
        "            \n",
        "        X = sample.drop([self.target], axis=1)\n",
        "        Y = sample[self.target]\n",
        "\n",
        "        s = cross_validate(clf, X, Y, scoring=['roc_auc'], cv=cv, n_jobs=-1)\n",
        "        self.cache[(m_name, df_name, sample_len, cv)] = s\n",
        "\n",
        "        return s\n",
        "\n",
        "    def runTests(self, sample_len=80000, cv=4):\n",
        "        # Tests the added models on all the added datasets\n",
        "        scores = {}\n",
        "        for m_name in self.models:\n",
        "            for df_name in self.datasets:\n",
        "                # print('Testing %s' % str((m_name, df_name)), end='')\n",
        "                start = time.time()\n",
        "\n",
        "                score = self.testModelWithDataset(m_name, df_name, sample_len, cv)\n",
        "                scores[(m_name, df_name)] = score\n",
        "                \n",
        "                end = time.time()\n",
        "                \n",
        "                # print(' -- %0.2fs ' % (end - start))\n",
        "\n",
        "        print('--- Top 10 Results ---')\n",
        "        for score in sorted(scores.items(), key=lambda x: -1 * x[1]['test_roc_auc'].mean())[:10]:\n",
        "            auc = score[1]['test_roc_auc']\n",
        "            print(\"%s --> AUC: %0.4f (+/- %0.4f)\" % (str(score[0]), auc.mean(), auc.std()))\n",
        "\n",
        "            \n",
        "# We will use a tester object across all models\n",
        "tester = Tester('SeriousDlqin2yrs')\n",
        "\n",
        "# You can add datasets like this:\n",
        "tester.addDataset('Drop Missing', df.dropna())\n",
        "\n",
        "# And models like this:\n",
        "rfc = RandomForestClassifier(n_estimators=15, max_depth = 6, random_state=0)\n",
        "tester.addModel('Simple Random Forest', rfc)\n",
        "tester.addModel('Simple SVM', svm.LinearSVC())\n",
        "\n",
        "# You can then use it to run the tests\n",
        "tester.runTests()"
      ],
      "id": "fe928bd1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7720ba3b"
      },
      "source": [
        "model.predict_proba(df) # show probablility in %\n",
        "model.classes_ # show for every probablity above name of class\n",
        "model.predict(df) # show the best probablity from all classes\n",
        "model.coef_ # show inpact every column on result but for FandomForrestRegressor work only:\n",
        "model.feautere_importances_"
      ],
      "id": "7720ba3b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3e23a92"
      },
      "source": [
        "Efficient function for checking different models"
      ],
      "id": "a3e23a92"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e246c7ce"
      },
      "source": [
        "# Define the models\n",
        "model_1 = RandomForestRegressor(n_estimators=50, random_state=0)\n",
        "model_2 = RandomForestRegressor(n_estimators=100, random_state=0)\n",
        "model_3 = RandomForestRegressor(n_estimators=100, criterion='mae', random_state=0)\n",
        "model_4 = RandomForestRegressor(n_estimators=200, min_samples_split=20, random_state=0)\n",
        "model_5 = RandomForestRegressor(n_estimators=100, max_depth=7, random_state=0)\n",
        "\n",
        "models = [model_1, model_2, model_3, model_4, model_5]"
      ],
      "id": "e246c7ce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b42e657a"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Function for comparing different models\n",
        "def score_model(model, X_t=X_train, X_v=X_valid, y_t=y_train, y_v=y_valid):\n",
        "    model.fit(X_t, y_t)\n",
        "    preds = model.predict(X_v)\n",
        "    return mean_absolute_error(y_v, preds)\n",
        "\n",
        "for i in range(0, len(models)):\n",
        "    mae = score_model(models[i])\n",
        "    print(\"Model %d MAE: %d\" % (i+1, mae))"
      ],
      "id": "b42e657a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "666d8b41"
      },
      "source": [
        "Nessesarily try Kfold I looked at one Kaggler"
      ],
      "id": "666d8b41"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aa90aed"
      },
      "source": [
        "\n",
        "scores = []\n",
        "feature_importance = pd.DataFrame()\n",
        "models = []\n",
        "columns = [col for col in train.columns if col not in ['id', 'breath_id', 'pressure']]\n",
        "X = train[columns]\n",
        "y = train['pressure']\n",
        "params = {'objective': 'regression',\n",
        "          'learning_rate': 0.3,\n",
        "          \"boosting_type\": \"gbdt\",\n",
        "          \"metric\": 'mae',\n",
        "          'n_jobs': -1,\n",
        "          'min_data_in_leaf':32,\n",
        "          'num_leaves':1024,\n",
        "         }\n",
        "folds = GroupKFold(n_splits=5)\n",
        "for fold_n, (train_index, valid_index) in enumerate(folds.split(train, y, groups=train['breath_id'])):\n",
        "    print(f'Fold {fold_n} started at {time.ctime()}')\n",
        "    X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
        "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "    model = lgb.LGBMRegressor(**params, n_estimators=10000)\n",
        "    model.fit(X_train, y_train, \n",
        "            eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "            verbose=1000, early_stopping_rounds=15)\n",
        "    score = metrics.mean_absolute_error(y_valid, model.predict(X_valid))\n",
        "    \n",
        "    models.append(model)\n",
        "    scores.append(score)\n",
        "\n",
        "    fold_importance = pd.DataFrame()\n",
        "    fold_importance[\"feature\"] = columns\n",
        "    fold_importance[\"importance\"] = model.feature_importances_\n",
        "    fold_importance[\"fold\"] = fold_n + 1\n",
        "    feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
        "print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))"
      ],
      "id": "7aa90aed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ba400cb"
      },
      "source": [
        "feature_importance[\"importance\"] /= 5\n",
        "cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
        "    by=\"importance\", ascending=False)[:50].index\n",
        "\n",
        "best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
        "\n",
        "plt.figure(figsize=(16, 12));\n",
        "sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
        "plt.title('LGB Features (avg over folds)');"
      ],
      "id": "4ba400cb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0edf7cc7"
      },
      "source": [
        "for model in models:\n",
        "    sub['pressure'] += model.predict(test[columns])\n",
        "sub['pressure'] /= 5\n",
        "sub.to_csv('first_sub.csv', index=False)"
      ],
      "id": "0edf7cc7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "692b23bb"
      },
      "source": [
        "# Submission for Kaggle"
      ],
      "id": "692b23bb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "824314a0"
      },
      "source": [
        "sub = pd.DataFrame()\n",
        "sub['Id'] = test_ID\n",
        "sub['SalePrice'] = ensemble\n",
        "sub.to_csv('submission.csv',index=False)"
      ],
      "id": "824314a0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5757f0b2"
      },
      "source": [
        "#Create DF and write outcome results\n",
        "classes = ['class1', 'class2', 'class3', 'class4','class5', 'class6', 'class7', 'class8','class9']\n",
        "submit = pd.DataFrame(y_pred, columns=classes)\n",
        "submit['ID'] = test['ID'].values\n",
        "submit = submit[['ID', 'class1', 'class2', 'class3', 'class4','class5', 'class6', 'class7', 'class8','class9']]"
      ],
      "id": "5757f0b2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a827ba2"
      },
      "source": [
        "# Data leakage"
      ],
      "id": "0a827ba2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cb6b35c"
      },
      "source": [
        "def lag_feature(df, lag, col, merge_cols):        \n",
        "    temp = df[merge_cols + [col]]\n",
        "    temp = temp.groupby(merge_cols).agg({f'{col}':'first'}).reset_index()\n",
        "    temp.columns = merge_cols + [f'{col}_lag{lag}']\n",
        "    temp['date_block_num'] += lag\n",
        "    df = pd.merge(df, temp, on=merge_cols, how='left')\n",
        "    df[f'{col}_lag{lag}'] = df[f'{col}_lag{lag}'].fillna(0).astype('float32')\n",
        "    return df\n",
        "#here more https://www.kaggle.com/deepdivelm/feature-engineering-lightgbm-exploring-performance#1.-Introduction"
      ],
      "id": "7cb6b35c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01654197"
      },
      "source": [
        "# My individual notes (are not copy pastes)"
      ],
      "id": "01654197"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62ec66fd"
      },
      "source": [
        "%%time\n",
        "#XGBRegressor\n",
        "import xgboost as xgb\n",
        "xgb = xgb.XGBRegressor()\n",
        "xgb.fit(X, y)"
      ],
      "id": "62ec66fd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba680677"
      },
      "source": [
        "%%time\n",
        "#LGBMRegressor\n",
        "import lightgbm as lgb\n",
        "lgb = lgb.LGBMRegressor()\n",
        "lgb.fit(X, y)"
      ],
      "id": "ba680677",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82fabf3b"
      },
      "source": [
        "%%time\n",
        "from sklearn.linear_model import LinearRegression\n",
        "linear = LinearRegression()\n",
        "linear.fit(X, y)"
      ],
      "id": "82fabf3b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "069f39e3"
      },
      "source": [
        "%%time\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rf = RandomForestRegressor()\n",
        "rf.fit(X, y)"
      ],
      "id": "069f39e3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "852f0953"
      },
      "source": [
        "%%time\n",
        "from sklearn.linear_model import Ridge\n",
        "ridge = Ridge()\n",
        "n_samples, n_features = 1, 6 #n_samples is amount of targets, n_features is amount of features\n",
        "rng = np.random.RandomState(0)\n",
        "y = rng.randn(n_samples)\n",
        "X = rng.randn(n_samples, n_features)\n",
        "ridge = Ridge(alpha=1.0)\n",
        "ridge.fit(X, y)"
      ],
      "id": "852f0953",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f715f07"
      },
      "source": [
        "StratifiedKFold - is exactly i can copy paste!"
      ],
      "id": "5f715f07"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccbab7c0"
      },
      "source": [
        "%%time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from xgboost import XGBClassifier \n",
        "\n",
        "folds = 5\n",
        "features = list(train_df.columns[1:119])\n",
        "\n",
        "train_oof = np.zeros((957919,)) # 957919 - is number train_df`s rows\n",
        "skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
        "for fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['claim'])):\n",
        "    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n",
        "    y_train = X_train['claim']\n",
        "    y_valid = X_valid['claim']\n",
        "    X_train = X_train.drop('claim', axis=1)\n",
        "    X_valid = X_valid.drop('claim', axis=1)\n",
        "\n",
        "    model = XGBClassifier(random_state=42, verbosity=0, tree_method='gpu_hist') # if such tree_method we need enable gpu accelerator\n",
        "\n",
        "    model =  model.fit(X_train, y_train, verbose=0)\n",
        "    temp_oof = model.predict_proba(X_valid)[:, 1]\n",
        "    train_oof[valid_idx] = temp_oof\n",
        "    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n",
        "    \n",
        "print(f'OOF AUC: ', roc_auc_score(train_df['claim'], train_oof))"
      ],
      "id": "ccbab7c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "298f5fef"
      },
      "source": [
        "# Metrics\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "model = clf.best_estimator_ # input model\n",
        "prediction = model.predict(X_test)\n",
        "print(\"MAE\", mean_absolute_error(y_test, prediction))"
      ],
      "id": "298f5fef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd7d0816"
      },
      "source": [
        "# Create prediction and submission \n",
        "def write_to_submission_file(predicted_labels, out_file,\n",
        "                             target='pressure', index_label=\"id\"): # write titlise of columns\n",
        "    predicted_df = pd.DataFrame(predicted_labels,\n",
        "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
        "                                columns=[target])\n",
        "    predicted_df.to_csv(out_file, index_label=index_label)\n",
        "\n",
        "test_pred = model.predict(test)\n",
        "s = 'subm1.csv' # write name of file \n",
        "write_to_submission_file(test_pred, s)\n",
        "subm = pd.read_csv(s)\n",
        "subm"
      ],
      "id": "bd7d0816",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1224efbd"
      },
      "source": [
        ""
      ],
      "id": "1224efbd",
      "execution_count": null,
      "outputs": []
    }
  ]
}